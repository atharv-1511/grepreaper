---
title: "Using grep_read: Efficient Data Filtering with grep"
author: "YAtharv Raskar"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using grep_read: Efficient Data Filtering with grep}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(grepreaper)
```

# Introduction

The `grep_read` package provides an efficient way to read and filter data from files using grep patterns. This vignette demonstrates how to use the package's features through practical examples.

## System Requirements

The package requires the `grep` command-line tool to be available on your system. Here's what you need to know for different operating systems:

### Windows
- Install Git for Windows (recommended) which includes Git Bash with grep
- Or install MSYS2 and run `pacman -S grep`
- Or use Windows Subsystem for Linux (WSL)

### MacOS
- grep is included by default
- If missing, install via Homebrew: `brew install grep`
- Or install via MacPorts: `sudo port install grep`

### Linux
- grep is typically pre-installed
- If missing, install using your package manager:
  - Debian/Ubuntu: `sudo apt-get install grep`
  - RHEL/CentOS: `sudo yum install grep`
  - Arch Linux: `sudo pacman -S grep`

<!--
You can check if grep is available on your system using:

```{r check_grep}
# Check grep availability
# grep_check <- check_grep_availability()
# print(grep_check)
```
-->

# Basic Usage

## Reading All Lines

You can read all lines from a file by using the default empty pattern:

```{r read_all}
# Create a sample CSV file
sample_data <- data.frame(
  department = c("IT", "HR", "IT", "Finance", "IT"),
  employee = c("John", "Alice", "Bob", "Charlie", "David"),
  salary = c(75000, 65000, 80000, 70000, 85000)
)
write.csv(sample_data, "sample_data.csv", row.names = FALSE)

# Read all lines
all_data <- grep_read("sample_data.csv")
print(all_data)
```

## Preserving Column Names

The function automatically preserves column names from the source file:

```{r column_names}
# Read with original column names
it_employees <- grep_read("sample_data.csv", "IT")
print(it_employees)

# Read with custom column names
custom_names <- c("dept", "name", "salary")
it_employees_custom <- grep_read("sample_data.csv", "IT", col.names = custom_names)
print(it_employees_custom)
```

## Reading from Multiple Files

When reading from multiple files, you can track the source of each row:

```{r multiple_files}
# Create two sample files
file1_data <- data.frame(
  department = c("IT", "HR"),
  employee = c("John", "Alice")
)
file2_data <- data.frame(
  department = c("IT", "Finance"),
  employee = c("Bob", "Charlie")
)

write.csv(file1_data, "file1.csv", row.names = FALSE)
write.csv(file2_data, "file2.csv", row.names = FALSE)

# Read from both files with source tracking
all_it_employees <- grep_read(
  c("file1.csv", "file2.csv"),
  "IT",
  include_filename = TRUE
)
print(all_it_employees)
```

## Progress Indicators

For large files, you can enable progress indicators:

```{r progress}
# Create a large sample file
large_data <- data.frame(
  department = rep(c("IT", "HR", "Finance"), 1000),
  employee = paste0("Employee", 1:3000),
  salary = sample(50000:100000, 3000)
)
write.csv(large_data, "large_data.csv", row.names = FALSE)

# Read with progress indicator
it_employees_large <- grep_read("large_data.csv", "IT", show_progress = TRUE)
print(head(it_employees_large))
```

## Line Numbers

You can include line numbers in the output:

```{r line_numbers}
# Read with line numbers
it_employees_with_lines <- grep_read("sample_data.csv", "IT", show_line_numbers = TRUE)
print(it_employees_with_lines)
```

## Count Only

Get a count of matching lines instead of the actual content:

```{r count_matches}
# Count IT department entries
it_count <- grep_read("sample_data.csv", "IT", count_only = TRUE)
print(it_count)

# Count matches across multiple files
multi_file_counts <- grep_read(
  c("file1.csv", "file2.csv"),
  "IT",
  count_only = TRUE
)
print(multi_file_counts)
```

## Reading All .csv Files in a Directory

You can use the `path` parameter to read all files in a directory (optionally recursively):

```{r read_path_example}
# Create a directory and some sample files
if (!dir.exists("data")) dir.create("data")
write.csv(data.frame(department = c("IT", "HR"), employee = c("John", "Alice")), "data/file1.csv", row.names = FALSE)
write.csv(data.frame(department = c("Finance", "IT"), employee = c("Bob", "Charlie")), "data/file2.csv", row.names = FALSE)

# Read all .csv files in the directory
data_all <- grep_read(path = "data", pattern = "IT", recursive = TRUE)
print(data_all)
```

# Advanced Features

## Pattern Matching Options

The function supports various pattern matching options:

```{r pattern_matching}
# Case-insensitive matching
it_employees_any_case <- grep_read("sample_data.csv", "it", ignore_case = TRUE)
print(it_employees_any_case)

# Word matching (exact words only)
similar_words <- data.frame(
  text = c("IT department", "FIT test", "IT", "HIT the target")
)
write.csv(similar_words, "similar_words.csv", row.names = FALSE)
whole_word_matches <- grep_read("similar_words.csv", "IT", word_match = TRUE)
print(whole_word_matches)

# Fixed string matching (no regex)
fixed_matches <- grep_read("sample_data.csv", "IT", fixed = TRUE)
print(fixed_matches)

# Inverted search (non-matching lines)
non_it_employees <- grep_read("sample_data.csv", "IT", invert = TRUE)
print(non_it_employees)
```

## Reading Large Files

For large files, you can use various options to control memory usage:

```{r large_files}
# Create a large sample file
large_data <- data.frame(
  department = rep(c("IT", "HR", "Finance"), 1000),
  employee = paste0("Employee", 1:3000),
  salary = sample(50000:100000, 3000)
)
write.csv(large_data, "large_data.csv", row.names = FALSE)

# Read with row limits
top_10_it <- grep_read("large_data.csv", "IT", nrows = 10)
print(top_10_it)

# Skip initial rows
skip_first_100 <- grep_read("large_data.csv", "IT", skip = 100)
print(head(skip_first_100))

# Disable progress for better performance
fast_read <- grep_read("large_data.csv", "IT", show_progress = FALSE)
print(head(fast_read))
```

## Recursive Directory Search

You can search through directories recursively:

```{r recursive_search}
# Create a directory structure
dir.create("data", showWarnings = FALSE)
file.copy("sample_data.csv", "data/sample_data.csv")
file.copy("file1.csv", "data/file1.csv")
file.copy("file2.csv", "data/file2.csv")

# Search recursively
recursive_results <- grep_read("data", "IT", recursive = TRUE)
print(recursive_results)

# Search recursively with line numbers
recursive_with_lines <- grep_read("data", "IT", recursive = TRUE, show_line_numbers = TRUE)
print(recursive_with_lines)
```

## Error Handling

The function provides clear error messages for common issues:

```{r error_handling}
# Non-existent file
tryCatch(
  grep_read("nonexistent.csv", "IT"),
  error = function(e) print(e$message)
)

# Invalid pattern
tryCatch(
  grep_read("sample_data.csv", pattern = c("a", "b")),
  error = function(e) print(e$message)
)

# Empty files vector
tryCatch(
  grep_read(character(0), "IT"),
  error = function(e) print(e$message)
)
```

## Combining Features

You can combine multiple features for powerful data extraction:

```{r combined_features}
# Read from multiple files with line numbers and custom column names
custom_names <- c("dept", "name", "pay")
multi_file_with_lines <- grep_read(
  c("file1.csv", "file2.csv"),
  "IT",
  include_filename = TRUE,
  show_line_numbers = TRUE,
  col.names = custom_names
)
print(multi_file_with_lines)

# Count matches with word matching
word_match_counts <- grep_read(
  "similar_words.csv",
  "IT",
  word_match = TRUE,
  count_only = TRUE
)
print(word_match_counts)

# Recursive search with case-insensitive matching
recursive_case_insensitive <- grep_read(
  "data",
  "it",
  recursive = TRUE,
  ignore_case = TRUE,
  show_line_numbers = TRUE
)
print(recursive_case_insensitive)
```

# Cleanup

```{r cleanup, include = FALSE}
# Remove sample files and directories
unlink(c("sample_data.csv", "file1.csv", "file2.csv", 
         "large_data.csv", "similar_words.csv"))
unlink("data", recursive = TRUE)
```

# Best Practices

## Performance Tips

1. Use `fixed = TRUE` when searching for exact strings (not patterns)
2. Use `word_match = TRUE` to avoid partial matches
3. Use `nrows` parameter to limit the number of rows read
4. Use `skip` parameter to skip initial rows
5. Use `count_only = TRUE` when you only need the number of matches
6. Disable progress indicators (`show_progress = FALSE`) for better performance in scripts
7. Use `ignore_case = TRUE` instead of case-insensitive patterns for better performance
8. Use `recursive = TRUE` with caution on large directory structures

```{r performance}
# Efficient reading with multiple optimizations
efficient_read <- grep_read(
  "sample_data.csv",
  "IT",
  fixed = TRUE,
  word_match = TRUE,
  nrows = 10,
  show_progress = FALSE,
  ignore_case = TRUE
)
print(efficient_read)
```

# Conclusion

The `grep_read` package provides a powerful and efficient way to filter and read data from files using grep patterns. It's particularly useful when:

- You need to filter data before reading it into memory
- You're working with large files and want to process only matching rows
- You need to search across multiple files
- You want to maintain the relationship between data and its source files
- You need to quickly count matching lines across files
- You want to preserve column names and structure of your data
- You need to search recursively through directories
- You want to combine multiple search features for complex data extraction
