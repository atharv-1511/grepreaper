---
title: "Using grep_read: Efficient Data Filtering with grep"
author: "Atharv Raskar and David Shilane"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using grep_read: Efficient Data Filtering with grep}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

# Install from local source if not installed
if (!requireNamespace("grepreaper", quietly = TRUE)) {
  devtools::install_local("..", upgrade = "never", quiet = TRUE)
}
library(grepreaper)
```

# Introduction

The `grepreaper` package provides an efficient way to read and filter data from files using grep patterns. This vignette demonstrates how to use the package's features through practical examples.

# Citation

To cite `grepreaper` in publications, use:

```{r citation, eval=FALSE}
citation("grepreaper")
```

**Authors:** Atharv Raskar and David Shilane

## System Requirements

The package requires the `grep` command-line tool to be available on your system. Here's what you need to know for different operating systems:

### Windows
- Install Git for Windows (recommended) which includes Git Bash with grep
- Or install MSYS2 and run `pacman -S grep`
- Or use Windows Subsystem for Linux (WSL)

### MacOS
- grep is included by default
- If missing, install via Homebrew: `brew install grep`
- Or install via MacPorts: `sudo port install grep`

### Linux
- grep is typically pre-installed
- If missing, install using your package manager:
  - Debian/Ubuntu: `sudo apt-get install grep`
  - RHEL/CentOS: `sudo yum install grep`
  - Arch Linux: `sudo pacman -S grep`

<!--
You can check if grep is available on your system using:

```{r check_grep}
# Check grep availability
# grep_check <- check_grep_availability()
# print(grep_check)
```
-->

# Basic Usage

## Reading All Lines

You can read all lines from a file by using the default empty pattern:

```{r read_all, eval = grepreaper::check_grep_availability()$available}
# Create a sample CSV file
sample_data <- data.frame(
  department = c("IT", "HR", "IT", "Finance", "IT"),
  employee = c("John", "Alice", "Bob", "Charlie", "David"),
  salary = c(75000, 65000, 80000, 70000, 85000)
)
write.csv(sample_data, "sample_data.csv", row.names = FALSE)

# Read all lines
all_data <- grep_read("sample_data.csv")
print(all_data)
```

## Preserving Column Names

The function automatically preserves column names from the source file. 

> **Note:** As of version 0.1.0, `grep_read()` robustly preserves column names even if the header row is not matched by grep. This prevents 'columns not found' errors when reading files like `diamonds.csv` or other data with headers.

```{r column_names, eval = grepreaper::check_grep_availability()$available}
# Read with original column names
it_employees <- grep_read("sample_data.csv", "IT")
print(it_employees)

# Read with custom column names
custom_names <- c("dept", "name", "salary")
it_employees_custom <- grep_read("sample_data.csv", "IT", col.names = custom_names)
print(it_employees_custom)
```

## Reading from Multiple Files

When reading from multiple files, you can track the source of each row:

```{r multiple_files, eval = grepreaper::check_grep_availability()$available}
# Create two sample files
file1_data <- data.frame(
  department = c("IT", "HR"),
  employee = c("John", "Alice")
)
file2_data <- data.frame(
  department = c("IT", "Finance"),
  employee = c("Bob", "Charlie")
)

write.csv(file1_data, "file1.csv", row.names = FALSE)
write.csv(file2_data, "file2.csv", row.names = FALSE)

# Read from both files with source tracking
all_it_employees <- grep_read(
  c("file1.csv", "file2.csv"),
  "IT",
  include_filename = TRUE
)
print(all_it_employees)
```

## Header Handling and Data Types

> **Note:**
> When reading from multiple files, `grep_read()` automatically removes all duplicate header rows that may appear in the output (i.e., the header from the second and subsequent files). This ensures that only the first header is used, and no erroneous header rows appear as data.
>
> After removing duplicate headers, `grep_read()` restores the correct data types for each column by performing a shallow read of the first file. This means that numeric columns remain numeric, and character columns remain character, even after combining data from multiple files.
>
> This robust handling prevents issues where all columns become character due to header row contamination, and ensures your data is ready for analysis immediately.

Example:

```{r multi_header_example, eval = grepreaper::check_grep_availability()$available}
# Simulate reading the same file twice
multi_dat <- grep_read(files = rep("sample_data.csv", 2))
# No duplicate header rows:
any(multi_dat$department == "department" & multi_dat$employee == "employee") # Should be FALSE
# Correct column types:
sapply(multi_dat, class)
```

## Progress Indicators

For large files, you can enable progress indicators:

```{r progress, eval = grepreaper::check_grep_availability()$available}
# Create a large sample file
large_data <- data.frame(
  department = rep(c("IT", "HR", "Finance"), 1000),
  employee = paste0("Employee", 1:3000),
  salary = sample(50000:100000, 3000)
)
write.csv(large_data, "large_data.csv", row.names = FALSE)

# Read with progress indicator
it_employees_large <- grep_read("large_data.csv", "IT", show_progress = TRUE)
print(head(it_employees_large))
```

## Line Numbers

You can include line numbers in the output:

```{r line_numbers, eval = grepreaper::check_grep_availability()$available}
# Read with line numbers
it_employees_with_lines <- grep_read("sample_data.csv", "IT", show_line_numbers = TRUE)
print(it_employees_with_lines)
```

> **Note:**
> The line number column is now named `line` for clarity.

## Count Only

Get a count of matching lines instead of the actual content:

```{r count_matches, eval = grepreaper::check_grep_availability()$available}
# Count IT department entries
it_count <- grep_read("sample_data.csv", "IT", count_only = TRUE)
print(it_count)

# Count matches across multiple files
multi_file_counts <- grep_read(
  c("file1.csv", "file2.csv"),
  "IT",
  count_only = TRUE
)
print(multi_file_counts)
```

## Reading All .csv Files in a Directory

You can use the `path` parameter to read all files in a directory (optionally recursively):

```{r read_path_example, eval = grepreaper::check_grep_availability()$available}
# Create a directory and some sample files
if (!dir.exists("data")) dir.create("data")
write.csv(data.frame(department = c("IT", "HR"), employee = c("John", "Alice")), "data/file1.csv", row.names = FALSE)
write.csv(data.frame(department = c("Finance", "IT"), employee = c("Bob", "Charlie")), "data/file2.csv", row.names = FALSE)

# Read all .csv files in the directory
data_all <- grep_read(path = "data", pattern = "IT", recursive = TRUE)
print(data_all)
```

# Advanced Features

## Pattern Matching Options

The function supports various pattern matching options:

```{r pattern_matching, eval = grepreaper::check_grep_availability()$available}
# Case-insensitive matching
it_employees_any_case <- grep_read("sample_data.csv", "it", ignore_case = TRUE)
print(it_employees_any_case)

# Word matching (exact words only)
similar_words <- data.frame(
  text = c("IT department", "FIT test", "IT", "HIT the target")
)
write.csv(similar_words, "similar_words.csv", row.names = FALSE)
whole_word_matches <- grep_read("similar_words.csv", "IT", word_match = TRUE)
print(whole_word_matches)

# Fixed string matching (no regex)
fixed_matches <- grep_read("sample_data.csv", "IT", fixed = TRUE)
print(fixed_matches)

# Inverted search (non-matching lines)
non_it_employees <- grep_read("sample_data.csv", "IT", invert = TRUE)
print(non_it_employees)
```

## Extracting Only the Matched Text

The `only_matching` parameter (`-o` in `grep`) is a powerful feature that allows you to extract only the parts of a line that match your pattern, rather than returning the full line. This is especially useful for data extraction tasks.

```{r only_matching_example, eval = grepreaper::check_grep_availability()$available}
# Create a file with complex strings
text_data <- data.frame(
  log_entry = c(
    "INFO: User 'alex' logged in from 192.168.1.10",
    "ERROR: Failed login for user 'beth' from 10.0.0.5",
    "INFO: User 'carl' accessed resource /api/v1/data",
    "WARN: Disk space low on /dev/sda1"
  )
)
write.csv(text_data, "text_data.csv", row.names = FALSE)

# Extract all usernames
usernames <- grep_read("text_data.csv", "user '[a-z]+'", only_matching = TRUE)
print(usernames)

# The output is a single-column data.table. We can clean it up further in R.
usernames$match <- gsub("user '|'", "", usernames$match)
print(usernames)

# You can also include the source filename
ip_addresses <- grep_read(
  "text_data.csv", 
  pattern = "[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+", 
  only_matching = TRUE,
  include_filename = TRUE
)
print(ip_addresses)
```

## Reading Large Files

For large files, you can use various options to control memory usage:

```{r large_files, eval = grepreaper::check_grep_availability()$available}
# Create a large sample file
large_data <- data.frame(
  department = rep(c("IT", "HR", "Finance"), 1000),
  employee = paste0("Employee", 1:3000),
  salary = sample(50000:100000, 3000)
)
write.csv(large_data, "large_data.csv", row.names = FALSE)

# Read only the first 10 matching rows
it_head <- grep_read("large_data.csv", "IT", nrows = 10)
print(it_head)

# Skip the first 5 matching rows
it_skip <- grep_read("large_data.csv", "IT", skip = 5)
print(head(it_skip))
```

## Showing the `grep` command

Sometimes, you may want to see the exact `grep` command that `grep_read` is using. This can be useful for debugging or learning. You can do this by setting `show_cmd = TRUE`.

```{r show_command, eval = grepreaper::check_grep_availability()$available}
# Show the command for a simple query
cmd_str <- grep_read(
  "sample_data.csv",
  pattern = "IT",
  ignore_case = TRUE,
  show_cmd = TRUE
)
print(cmd_str)

# Show the command for a more complex query
cmd_str_complex <- grep_read(
  files = c("file1.csv", "file2.csv"),
  pattern = "finance",
  ignore_case = TRUE,
  invert = TRUE,
  show_cmd = TRUE
)
print(cmd_str_complex)
```

## Recursive Search

The `recursive` parameter allows you to search for files in subdirectories.

```{r recursive_search, eval = grepreaper::check_grep_availability()$available}
# Create nested directories and files for recursive search
if (!dir.exists("data/subdir")) dir.create("data/subdir")
write.csv(data.frame(department = "IT", employee = "Eve"), "data/subdir/file3.csv", row.names = FALSE)

# Search recursively for "IT"
recursive_data <- grep_read("data", "IT", recursive = TRUE)
print(recursive_data)
```

## Error Handling

The function provides clear error messages for common issues:

```{r error_handling}
# Non-existent file
tryCatch(
  grep_read("nonexistent.csv", "IT"),
  error = function(e) print(e$message)
)

# Invalid pattern
tryCatch(
  grep_read("sample_data.csv", pattern = c("a", "b")),
  error = function(e) print(e$message)
)

# Empty files vector
tryCatch(
  grep_read(character(0), "IT"),
  error = function(e) print(e$message)
)
```

## Combining Features

You can combine multiple features for powerful data extraction:

```{r combined_features, eval = grepreaper::check_grep_availability()$available}
# Find all non-IT employees in all .csv files in the data directory,
# showing line numbers, ignoring case, and tracking the filename.
complex_query_results <- grep_read(
  path = "data",
  file_pattern = "\\.csv$",
  pattern = "it",
  invert = TRUE,
  ignore_case = TRUE,
  show_line_numbers = TRUE,
  include_filename = TRUE,
  recursive = TRUE
)
print(complex_query_results)
```

## Filtering Files by Pattern (file_pattern is optional)

You can use the `file_pattern` argument to filter which files in a directory are read. For example, to only include `.csv` files:

```{r file_pattern_example, eval = grepreaper::check_grep_availability()$available}
csv_data <- grep_read(path = "data", file_pattern = "*.csv", pattern = "IT")
print(csv_data)
```

If `file_pattern` is omitted, all files in the directory will be included:

```{r file_pattern_omitted, eval = grepreaper::check_grep_availability()$available}
all_files_data <- grep_read(path = "data", pattern = "IT")
print(all_files_data)
```

# Troubleshooting

If you encounter errors such as:

    grepreaper failed: columns not found: [carat, cut, color, ...]

- Ensure your data file has a header row (column names) as the first line.
- Make sure the file is not empty and is a valid CSV.
- If you edited the file manually, check for extra blank lines or missing headers.
- If you are using a pattern that matches only data rows, `grep_read()` will still use the header from the file for column names.
- If you still have issues, try reading the file with `data.table::fread()` directly to check for format problems.

# Cleaning Up

```{r cleanup, eval = grepreaper::check_grep_availability()$available}
# Clean up created files and directories
unlink(c("sample_data.csv", "file1.csv", "file2.csv", "large_data.csv", "similar_words.csv", "text_data.csv"))
unlink("data", recursive = TRUE)
```

# Best Practices

## Performance Tips

1. Use `fixed = TRUE` when searching for exact strings (not patterns)
2. Use `word_match = TRUE` to avoid partial matches
3. Use `nrows` parameter to limit the number of rows read
4. Use `skip` parameter to skip initial rows
5. Use `count_only = TRUE` when you only need the number of matches
6. Disable progress indicators (`show_progress = FALSE`) for better performance in scripts
7. Use `ignore_case = TRUE` instead of case-insensitive patterns for better performance
8. Use `recursive = TRUE` with caution on large directory structures

```{r performance, eval = grepreaper::check_grep_availability()$available}
# Efficient reading with multiple optimizations
efficient_read <- grep_read(
  "sample_data.csv",
  "IT",
  fixed = TRUE,
  word_match = TRUE,
  nrows = 10,
  show_progress = FALSE,
  ignore_case = TRUE
)
print(efficient_read)
```

# Conclusion

The `grep_read` package provides a powerful and efficient way to filter and read data from files using grep patterns. It's particularly useful when:

- You need to filter data before reading it into memory
- You're working with large files and want to process only matching rows
- You need to search across multiple files
- You want to maintain the relationship between data and its source files
- You need to quickly count matching lines across files
- You want to preserve column names and structure of your data
- You need to search recursively through directories
- You want to combine multiple search features for complex data extraction

# Session Info

```{r session_info}
sessionInfo()
```
